import warnings
warnings.filterwarnings("ignore")

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(0)   # Do not change this value: required to be compatible with solutions generated by the autograder.

def engagement_model():
    # Step 1: Load data
    train = pd.read_csv("assets/train.csv")
    test = pd.read_csv("assets/test.csv")

    # Step 2: Separate features and label
    X_train = train.drop(columns=["engagement", "id"])  # All features
    y_train = train["engagement"]                      # Target: True/False

    X_test = test.drop(columns=["id"])                 # Only features

    # Step 3: Import model
    from sklearn.ensemble import RandomForestClassifier

    # Step 4: Train model
    model = RandomForestClassifier(n_estimators=100, random_state=0)
    model.fit(X_train, y_train)

    # Step 5: Predict probabilities on test set
    y_probs = model.predict_proba(X_test)[:, 1]  # Get probability for class "True"

    # Step 6: Return a Pandas Series with id as index
    result = pd.Series(data=y_probs, index=test["id"].astype(int), name="engagement")
    return result

stu_ans = engagement_model()
assert isinstance(stu_ans, pd.Series), "Your function should return a pd.Series. "
assert len(stu_ans) == 2309, "Your series is of incorrect length: expected 2309 "
assert np.issubdtype(stu_ans.index.dtype, np.integer), "Your answer pd.Series should have an index of integer type representing video id."



stu_ans = engagement_model()
print("Index type:", stu_ans.index.dtype)
print("First 5 predictions:\n", stu_ans.head())



from sklearn.metrics import roc_curve, roc_auc_score

# Reload training data
train = pd.read_csv("assets/train.csv")
X_train = train.drop(columns=["engagement", "id"])
y_train = train["engagement"]

# Train the same model again on training data
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100, random_state=0)
model.fit(X_train, y_train)

# Predict probabilities on training set
train_probs = model.predict_proba(X_train)[:, 1]

# Compute FPR, TPR for ROC
fpr, tpr, _ = roc_curve(y_train, train_probs)
auc_score = roc_auc_score(y_train, train_probs)

# Plot ROC
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.2f})', color='blue')
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Video Engagement Prediction (Train Data)')
plt.legend()
plt.grid(True)
plt.show()

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# Reload training data
train = pd.read_csv("assets/train.csv")
X_train = train.drop(columns=["engagement", "id"])
y_train = train["engagement"]

params = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
}

grid = GridSearchCV(RandomForestClassifier(random_state=0),
                    param_grid=params,
                    scoring='roc_auc',
                    cv=3,
                    n_jobs=-1)

grid.fit(X_train, y_train)

print("Best parameters:", grid.best_params_)
print("Best AUC:", grid.best_score_)

from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score

# Reload training data
train = pd.read_csv("assets/train.csv")
X_train = train.drop(columns=["engagement", "id"])
y_train = train["engagement"]

xgb = XGBClassifier(random_state=0, use_label_encoder=False, eval_metric='logloss')
xgb.fit(X_train, y_train)

xgb_probs = xgb.predict_proba(X_train)[:, 1]
xgb_auc = roc_auc_score(y_train, xgb_probs)

print("XGBoost AUC:", xgb_auc)

# Step 1: Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier

# Step 2: Load the dataset
train = pd.read_csv("assets/train.csv")

# Step 3: Separate features and target
X_train = train.drop(columns=["engagement", "id"])
y_train = train["engagement"]

# Step 4: Train Random Forest model
model = RandomForestClassifier(n_estimators=100, random_state=0)
model.fit(X_train, y_train)

# Step 5: Get feature importances
importances = model.feature_importances_
feature_names = X_train.columns
feature_importance_series = pd.Series(importances, index=feature_names).sort_values(ascending=True)

# Step 6: Plot feature importances as a horizontal bar chart
plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importance_series, y=feature_importance_series.index, palette="viridis")
plt.title("Feature Importance (Random Forest)")
plt.xlabel("Importance Score")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

print(feature_importance_series.sort_values(ascending=False))

# Step 1: Install xgboost if not already installed (uncomment if needed)
# !pip install xgboost

# Step 2: Import libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from xgboost import XGBClassifier

# Step 3: Load data
train = pd.read_csv("assets/train.csv")
X_train = train.drop(columns=["engagement", "id"])
y_train = train["engagement"]

# Step 4: Train the XGBoost model
xgb = XGBClassifier(random_state=0, use_label_encoder=False, eval_metric='logloss')
xgb.fit(X_train, y_train)

# Step 5: Get feature importances
xgb_importances = xgb.feature_importances_
feature_names = X_train.columns
xgb_importance_series = pd.Series(xgb_importances, index=feature_names).sort_values(ascending=True)

# Step 6: Plot the importances
plt.figure(figsize=(10, 6))
sns.barplot(x=xgb_importance_series, y=xgb_importance_series.index, palette="mako")
plt.title("Feature Importance (XGBoost)")
plt.xlabel("Importance Score")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

print("Random Forest Feature Importance:")
print(feature_importance_series.sort_values(ascending=False))

print("\nXGBoost Feature Importance:")
print(xgb_importance_series.sort_values(ascending=False))

# Step 1: Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier

# Step 2: Load dataset
train = pd.read_csv("assets/train.csv")
X = train.drop(columns=["engagement", "id"])
y = train["engagement"]

# Step 3: Train Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=0)
rf.fit(X, y)
rf_importance = pd.Series(rf.feature_importances_, index=X.columns)

# Step 4: Train XGBoost
xgb = XGBClassifier(random_state=0, use_label_encoder=False, eval_metric='logloss')
xgb.fit(X, y)
xgb_importance = pd.Series(xgb.feature_importances_, index=X.columns)

# Step 5: Combine importances
comparison_df = pd.DataFrame({
    "Random Forest": rf_importance,
    "XGBoost": xgb_importance
}).sort_values(by="Random Forest", ascending=False)

# Step 6: Plot Top 10 Features Side-by-Side
top_n = 10
top_features = comparison_df.head(top_n)

x = np.arange(len(top_features))
width = 0.35

plt.figure(figsize=(12, 6))
plt.barh(x - width/2, top_features["Random Forest"], height=width, label="Random Forest", color="skyblue")
plt.barh(x + width/2, top_features["XGBoost"], height=width, label="XGBoost", color="salmon")

plt.yticks(x, top_features.index)
plt.xlabel("Importance Score")
plt.title("Side-by-Side Feature Importance Comparison (Top 10 Features)")
plt.gca().invert_yaxis()
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

